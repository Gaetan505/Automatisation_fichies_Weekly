{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## Bibliothèques nécéssaires \n",
    "##############################\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import PyPDF2\n",
    "import base64\n",
    "import logging\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from openpyxl import load_workbook\n",
    "from google.cloud import secretmanager\n",
    "from datetime import datetime, timedelta\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload\n",
    "import ast\n",
    "from google.oauth2.credentials import Credentials as Credentials_user\n",
    "from google.oauth2.service_account import Credentials as Credentials_sa\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FOLDER_NAME, FILENAME = \"NE PAS SUPPRIMER\", \"processed_ids.json\"\n",
    "TARGET_DRIVE_FOLDER_ID = \"13_F3h2Elh4JCW50JvPtvX19l8ZstmIny\"\n",
    "\n",
    "\n",
    "## Authentification\n",
    "\n",
    "def get_creds_mail():\n",
    "    gcp_secret_client = secretmanager.SecretManagerServiceClient()\n",
    "    secret_resource_name = \"projects/437392419716/secrets/o2c-ca-token/versions/1\"\n",
    "    response = gcp_secret_client.access_secret_version(request={\"name\": secret_resource_name})\n",
    "    token = ast.literal_eval(response.payload.data.decode(\"UTF-8\"))\n",
    "    return Credentials_user.from_authorized_user_info(token)\n",
    "\n",
    "def get_creds():\n",
    "    gcp_secret_client = secretmanager.SecretManagerServiceClient()\n",
    "    secret_resource_name = \"projects/437392419716/secrets/sa_o2c_relance_client_private_key/versions/1\"\n",
    "    response = gcp_secret_client.access_secret_version(request={\"name\": secret_resource_name})\n",
    "    token = ast.literal_eval(response.payload.data.decode(\"UTF-8\"))\n",
    "    return Credentials_sa.from_service_account_info(token)\n",
    "\n",
    "\n",
    "CREDS = get_creds()\n",
    "CREDS_MAIL = get_creds_mail()\n",
    "\n",
    "#  Initialisation des services Drive et Gmail\n",
    "try:\n",
    "    drive_service = build('drive', 'v3', credentials=CREDS, cache_discovery=False)\n",
    "    gmail_service = build('gmail', 'v1', credentials=CREDS_MAIL, cache_discovery=False)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erreur lors de la création des services Google API : {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "## fichier des log pour debugging\n",
    "#################################\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    filename='extraction.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logging.info(\"Début du traitement...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "## 1ère partie  fonctions  ##\n",
    "#############################\n",
    "\n",
    "# nettoyer les noms pour une comparaison optimale\n",
    "def normaliser_nom(nom):\n",
    "    if pd.isnull(nom):\n",
    "        return None\n",
    "    nom = str(nom)\n",
    "    nom = unicodedata.normalize('NFD', nom).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    nom = nom.upper()\n",
    "    nom = re.sub(r'[^\\w\\s]', ' ', nom)\n",
    "    nom = ' '.join(nom.split())\n",
    "    return nom\n",
    "\n",
    "\n",
    "\n",
    "# telecharger les fichiers sur le drive \n",
    "def download_file_from_drive(service, file_id):\n",
    "    file_stream = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(file_stream, service.files().get_media(fileId=file_id))\n",
    "    done = False\n",
    "    while not done:\n",
    "        _, done = downloader.next_chunk()\n",
    "    file_stream.seek(0)\n",
    "    return file_stream\n",
    "\n",
    "\n",
    "\n",
    "#Recherche un fichier sur Google Drive par son nom et son type MIME, dans un dossier donné si précisé.\n",
    "#Si le fichier existe, retourne son ID ; sinon, le crée et retourne l'ID du fichier créé.\n",
    "def get_or_create(service, name, mime, parent=None, content=None):\n",
    "    q = f\"name='{name}' and mimeType='{mime}'\" + (f\" and '{parent}' in parents\" if parent else \"\")\n",
    "    r = service.files().list(\n",
    "        q=q,\n",
    "        spaces='drive',\n",
    "        fields='files(id)',\n",
    "        supportsAllDrives=True,               \n",
    "        includeItemsFromAllDrives=True        \n",
    "    ).execute().get('files', [])\n",
    "\n",
    "    if r:\n",
    "        return r[0]['id']\n",
    "\n",
    "    meta = {'name': name, 'mimeType': mime} | ({'parents': [parent]} if parent else {})\n",
    "    media = MediaIoBaseUpload(BytesIO(content.encode()) if content else None, mimetype=\"application/json\") if content else None\n",
    "    return service.files().create(\n",
    "        body=meta,\n",
    "        media_body=media,\n",
    "        fields='id',\n",
    "        supportsAllDrives=True                \n",
    "    ).execute()['id']\n",
    "\n",
    "\n",
    "\n",
    "#Charge l'état depuis le fichier Drive : les IDs déjà traités et la dernière date de traitement.\n",
    "#Si le fichier est vide, retourne un état initial avec un set vide et une date par défaut.\n",
    "\n",
    "def load_state(service, fid):\n",
    "    req, buf = service.files().get_media(fileId=fid), BytesIO()\n",
    "    downloader = MediaIoBaseDownload(buf, req)\n",
    "    done = False\n",
    "    while not done:\n",
    "        _, done = downloader.next_chunk()\n",
    "    buf.seek(0)\n",
    "    content = buf.read()\n",
    "    if not content:\n",
    "        return set(), \"2025/04/29\"  #  Date de départ si fichier vide\n",
    "\n",
    "    data = json.loads(content)\n",
    "    ids = set(data.get(\"processed_ids\", []))\n",
    "    last_date = data.get(\"last_processed_date\", \"2025/04/29\")\n",
    "    return ids, last_date\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#Sauvegarde l'état : les IDs déjà traités et la dernière date de traitement.\n",
    "#Écrase le contenu du fichier sur Drive. \n",
    "def save_state(service, fid, ids, last_date):\n",
    "    data = {\n",
    "        \"processed_ids\": list(ids),\n",
    "        \"last_processed_date\": last_date\n",
    "    }\n",
    "    media = MediaIoBaseUpload(BytesIO(json.dumps(data).encode()), mimetype=\"application/json\")\n",
    "    service.files().update(fileId=fid, media_body=media).execute()\n",
    "\n",
    "\n",
    "\n",
    "# Force la mise à jour de la date dans le fichier  sur Drive \n",
    "def force_update_state(service, file_id, forced_date):\n",
    "    data = {\n",
    "        \"processed_ids\": [],  \n",
    "        \"last_processed_date\": forced_date\n",
    "    }\n",
    "    media = MediaIoBaseUpload(BytesIO(json.dumps(data).encode()), mimetype=\"application/json\")\n",
    "    service.files().update(fileId=file_id, media_body=media).execute()\n",
    "    logging.info(f\"Date forcée à : {forced_date}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_csvs_from_gmail(service, query, ids):\n",
    "    results = service.users().messages().list(userId=\"me\", q=query).execute()\n",
    "    new_ids, csvs = set(), []\n",
    "    for msg in results.get(\"messages\", []):\n",
    "        mid = msg[\"id\"]\n",
    "        if mid in ids: continue\n",
    "        msg_data = service.users().messages().get(userId=\"me\", id=mid).execute()\n",
    "        for p in msg_data[\"payload\"].get(\"parts\", []):\n",
    "            if p.get(\"filename\", \"\").endswith(\".csv\") and \"attachmentId\" in p[\"body\"]:\n",
    "                data = service.users().messages().attachments().get(userId=\"me\", messageId=mid, id=p[\"body\"][\"attachmentId\"]).execute()\n",
    "                s = BytesIO(base64.urlsafe_b64decode(data[\"data\"])); s.name = p[\"filename\"]\n",
    "                csvs.append({\"stream\": s, \"message_id\": mid}); new_ids.add(mid)\n",
    "    return csvs, new_ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Téléchargement des fichiers sur le drive  et transformation en dataframe\n",
    "BD_Couleur=pd.read_excel(download_file_from_drive(drive_service, '1y_eu0l2ZrqQH7sWn6-HOlE_FHyL5afJA'), engine='openpyxl')\n",
    "BD_VIOLETTE=pd.read_excel(download_file_from_drive(drive_service, '1K9KMDL2QyCUprcWIWxZz_E9vtomxm4z7'), engine='openpyxl')\n",
    "BD_ROUGE=pd.read_excel(download_file_from_drive(drive_service, '1riHpyh_wBvIOjGSB7ykF2TmpDNFcnvUr'), engine='openpyxl')\n",
    "BD_ORANGE=pd.read_excel(download_file_from_drive(drive_service, '16BsnVrVZXANsNJ8vR2A1x1p1Pwgb5m2X'), engine='openpyxl')\n",
    "\n",
    "\n",
    "# Création / récupération du dossier et du fichier JSON\n",
    "folder_id = get_or_create(drive_service, FOLDER_NAME, 'application/vnd.google-apps.folder')\n",
    "file_id = get_or_create(drive_service, FILENAME, 'application/json', folder_id, content='{\"processed_ids\": [], \"last_processed_date\": \"2025/01/01\"}')\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "## Pour forcer la date \n",
    "forcer_date = True  # Mettre à True pour forcer la date\n",
    "\n",
    "if forcer_date:\n",
    "    force_update_state(drive_service, file_id, \"2025/05/08\")  # mettre la date voulue ici\n",
    "##\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Charger l'état (IDs + date)\n",
    "try:\n",
    "    ids, last_processed_date = load_state(drive_service, file_id)\n",
    "    logging.info(f\"{len(ids)} IDs déjà traités. Dernière date de traitement : {last_processed_date}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erreur lors du chargement de l'état depuis Drive : {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "# Construire la requête avec la dernière date\n",
    "query = f\"from:automail@epay.de filename:csv after:{last_processed_date}\"\n",
    "#query = f\"from:automail@epay.de filename:csv after:{last_processed_date}\"\n",
    "#query = f\"in:anywhere from:gaetan_djambissie_nana@carrefour.com filename:csv after:{last_processed_date}\"\n",
    "\n",
    "\n",
    "\n",
    "# Chercher les nouveaux CSVs depuis Gmail\n",
    "try:\n",
    "    csvs, new_ids = get_csvs_from_gmail(gmail_service, query, ids)\n",
    "    logging.info(f\"{len(csvs)} nouveaux fichiers CSV depuis Gmail.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erreur lors de la récupération des CSV depuis Gmail : {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "# Si des nouveaux mails, mettre à jour l'état\n",
    "if new_ids:\n",
    "    ids.update(new_ids)\n",
    "    today_str = datetime.now().strftime(\"%Y/%m/%d\")  \n",
    "    save_state(drive_service, file_id, ids, today_str)\n",
    "    logging.info(f\"État mis à jour avec {len(new_ids)} nouveaux IDs et nouvelle date : {today_str}\")\n",
    "else:\n",
    "    logging.info(\"Aucun nouveau mail à traiter. Pas de mise à jour nécessaire.\")\n",
    "\n",
    "# FILTRE les fichiers dont le nom contient 'DailySCC_Orders'\n",
    "matching_csvs = [c for c in csvs if \"DailySCC_Orders\" in (c['stream'].name or \"\")]\n",
    "logging.info(f\"{len(matching_csvs)} fichiers correspondent à 'DailySCC_Orders'.\")\n",
    "\n",
    "\n",
    "###############################################################\n",
    "## TRAITEMENT DES FICHIERS DAILY \n",
    "###############################################################\n",
    "\n",
    "for c in matching_csvs:\n",
    "    stream = c['stream']\n",
    "    msg_id = c['message_id']\n",
    "    stream.seek(0)\n",
    "\n",
    "    filename = stream.name\n",
    "\n",
    "        # Enlève l'extension convertit en date et ajoute 1 jour\n",
    "    if '.' in filename:\n",
    "        date_str = filename.split('_')[-1].split('.')[0].strip()\n",
    "    else:\n",
    "        date_str = filename.split('_')[-1].strip()\n",
    "\n",
    "    date_obj = datetime.strptime(date_str, '%Y%m%d')  \n",
    "    next_day = date_obj + timedelta(days=1)          \n",
    "    sem = next_day.strftime('%d-%m-%Y')              \n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(stream, encoding='ISO-8859-1', sep=';')\n",
    "        logging.info(f\"Fichier : {stream.name} - {len(df)} lignes\")\n",
    "\n",
    "        colonnes_a_nettoyer = [\n",
    "        \"Montant des cartes\",\n",
    "        \"Remise\",\n",
    "        \"Frais livraison HT\",\n",
    "        \"Frais additionnels\",\n",
    "        \"TVA totale\",\n",
    "        \"Montant total\"\n",
    "         ]\n",
    "        \n",
    "        for col in colonnes_a_nettoyer:\n",
    "            df.loc[:, col] = pd.to_numeric(\n",
    "                df[col].astype(str).str.replace(\",\", \"\").str.strip(),\n",
    "                errors='coerce'\n",
    "            ) \n",
    "\n",
    "\n",
    "        weekly = df[df[\"Commande API\"] == \"Non\"].copy()\n",
    "        \n",
    "\n",
    "        \n",
    "        # Séparation CB vs sans CB\n",
    "        weekly_CB = weekly[weekly[\"Moyen de paiement\"] == \"Carte bleue\"]\n",
    "        weekly_sans_CB = weekly[weekly[\"Moyen de paiement\"] != \"Carte bleue\"].copy()\n",
    "\n",
    "        # nettoyage des différents DataFrames\n",
    "        BD_Couleur['Nom Société clean'] = BD_Couleur['Nom Société'].apply(normaliser_nom)\n",
    "        weekly_sans_CB['Nom Société clean'] = weekly_sans_CB['Nom Société'].apply(normaliser_nom)\n",
    "        BD_VIOLETTE['Nom Société clean'] = BD_VIOLETTE['Nom Société'].apply(normaliser_nom)\n",
    "        BD_ROUGE['Nom Société clean'] = BD_ROUGE['Nom Société'].apply(normaliser_nom)\n",
    "        BD_ORANGE['Nom Société clean'] = BD_ORANGE['Nom Société'].apply(normaliser_nom)\n",
    "        \n",
    "        # Extraction des HYPER\n",
    "        noms_hyper = BD_Couleur[BD_Couleur[\"type de facture\"] == \"HYPER\"]['Nom Société clean'].unique()\n",
    "        weekly_hyper = weekly_sans_CB[weekly_sans_CB['Nom Société clean'].isin(noms_hyper)].copy()\n",
    "\n",
    "        # Ajout du compte comptable et du centre de coût aux HYPER \n",
    "        compte_dict = BD_VIOLETTE.set_index('Nom Société clean')['Compte comptable-D SAP'].to_dict()\n",
    "        weekly_hyper['Compte comptable-D SAP'] = weekly_hyper['Nom Société clean'].map(compte_dict)\n",
    "\n",
    "        centre_cout_dict = BD_VIOLETTE.set_index('Nom Société clean')['Centre de coût-D'].to_dict()\n",
    "        weekly_hyper['Centre de coût-D'] = weekly_hyper['Nom Société clean'].map(centre_cout_dict)\n",
    "\n",
    "        # tri par rapport au nom de société puis si identiques par rapport au numero commande \n",
    "        weekly_hyper = weekly_hyper.sort_values(\n",
    "        by=[\"Nom Société\", \"Numero Commande\"],\n",
    "        key=lambda col: col.str.casefold() if col.name == \"Nom Société\" else col)\n",
    "\n",
    "\n",
    "        # Extraction des Oranges\n",
    "        societes_orange = BD_ORANGE['Nom Société clean'].unique()\n",
    "        weekly_Orange = weekly_sans_CB[weekly_sans_CB['Nom Société clean'].isin(societes_orange)].copy()\n",
    "        # weekly_Orange['Date'] = pd.to_datetime(weekly_Orange['Date']).dt.date\n",
    "        weekly_Orange['Date'] = pd.to_datetime(weekly_Orange['Date'], errors='coerce').dt.strftime('%d.%m.%Y')\n",
    "\n",
    "        # Convertir les montants en nombre si ce n’est pas déjà fait\n",
    "        weekly_Orange['Montant total'] = pd.to_numeric(weekly_Orange['Montant total'], errors='coerce')\n",
    "\n",
    "        # Formater avec 2 décimales et remplacer le point par une virgule\n",
    "        weekly_Orange['Montant total'] = weekly_Orange['Montant total'].apply(\n",
    "        lambda x: f\"{x:.2f}\".replace('.', ',') if pd.notna(x) else '')\n",
    "\n",
    "        # Ajout des colonnes BU-D   CLIENT acheteur aux ORANGES \n",
    "        BU_dict = BD_ORANGE.set_index('Nom Société clean')['BU-D'].to_dict()\n",
    "        weekly_Orange['BU-D'] = weekly_Orange['Nom Société clean'].map(BU_dict)\n",
    "\n",
    "        cl_dict = BD_ORANGE.set_index('Nom Société clean')['CLIENT acheteur'].to_dict()\n",
    "        weekly_Orange['CLIENT acheteur'] = weekly_Orange['Nom Société clean'].map(cl_dict)\n",
    "        \n",
    "        weekly_Orange = weekly_Orange.sort_values(by='Nom Société')\n",
    "      \n",
    "\n",
    "        fichier_weekly_Orange = weekly_Orange.drop(columns=[\"Nom Société clean\"])\n",
    "\n",
    "\n",
    "        # Extraction des  Rouges\n",
    "        societes_rouge = BD_ROUGE['Nom Société clean'].unique()\n",
    "        weekly_Rouge = weekly_sans_CB[weekly_sans_CB['Nom Société clean'].isin(societes_rouge)].copy()\n",
    "        # weekly_Rouge['Date'] = pd.to_datetime(weekly_Rouge['Date']).dt.date\n",
    "        weekly_Rouge['Date'] = pd.to_datetime(weekly_Rouge['Date'], errors='coerce').dt.strftime('%d.%m.%Y')\n",
    "\n",
    "        # Convertir les montants en nombre si ce n’est pas déjà fait\n",
    "        weekly_Rouge['Montant total'] = pd.to_numeric(weekly_Rouge['Montant total'], errors='coerce')\n",
    "\n",
    "        # Formater avec 2 décimales et remplacer le point par une virgule\n",
    "        weekly_Rouge['Montant total'] = weekly_Rouge['Montant total'].apply(\n",
    "        lambda x: f\"{x:.2f}\".replace('.', ',') if pd.notna(x) else '')\n",
    "\n",
    "\n",
    "\n",
    "        # Ajout des colonnes CLIENT acheteur  CP (centre profit)    CC (centre coût) aux ROUGES \n",
    "        client_dict = BD_ROUGE.set_index('Nom Société clean')['CLIENT acheteur'].to_dict()\n",
    "        weekly_Rouge['CLIENT acheteur'] = weekly_Rouge['Nom Société clean'].map(client_dict)\n",
    "\n",
    "        cp_dict = BD_ROUGE.set_index('Nom Société clean')['CP (centre profit)'].to_dict()\n",
    "        weekly_Rouge['CP (centre profit)'] = weekly_Rouge['Nom Société clean'].map(cp_dict)\n",
    "\n",
    "        cc_dict = BD_ROUGE.set_index('Nom Société clean')['CC (centre coût)'].to_dict()\n",
    "        weekly_Rouge['CC (centre coût)'] = weekly_Rouge['Nom Société clean'].map(cc_dict)\n",
    "\n",
    "        weekly_Rouge = weekly_Rouge.sort_values(by='Nom Société')\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        fichier_weekly_Rouge = weekly_Rouge.drop(columns=[\"Nom Société clean\"])\n",
    "\n",
    "\n",
    "        # 3. Extraire la liste des sociétés connues\n",
    "        societes_connues = BD_Couleur['Nom Société clean'].unique()\n",
    "        weekly_NOIR = weekly_sans_CB[~weekly_sans_CB['Nom Société clean'].isin(societes_connues)].copy()\n",
    "\n",
    "        # tri par rapport au nom de société puis si identiques par rapport au numero commande \n",
    "        weekly_NOIR = weekly_NOIR.sort_values(\n",
    "        by=[\"Nom Société\", \"Numero Commande\"],\n",
    "        key=lambda col: col.str.casefold() if col.name == \"Nom Société\" else col)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        # Etape de  remplissage des differents fichiers \n",
    "        # entete pour les fichiers de  lignes verte et violette \n",
    "        colonnes = [\n",
    "            \"Date comptable\",\"Ledger\",\"Code Société\", \"Centre de profit\",\"Domaine d'activité\",\"Centre de coût\",\"Élément d'OTP\",\"Compte comptable\",\"Débit/Crédit\",\"Montant\",\n",
    "            \"Description écriture\",\"Code TVA\", \"Code Business Partner\",\"Code CGS\",\"Code Société SL partenaire\",\"Centre de profit SL partenaire\",\"Clé de lettrage\",\n",
    "            \"Nature de retraitement\",\"Code flux\",\"Code AFB\",\"Millésime\",\"Nature Fiscale\",\"Nature Reprise\",\"Clé Complémentaire\",\"Affectation\"\n",
    "        ]\n",
    "\n",
    "    # Remplissage  des OD VERTE\n",
    "        compte_debit =  2830200000\n",
    "        compte_credit=  2109000000\n",
    "\n",
    "        maquette_data = []\n",
    "        for sens in ['D', 'C']:\n",
    "            for i in range(len(weekly_CB)):\n",
    "                row = {col: '' for col in colonnes}\n",
    "                date_val = weekly_CB.iloc[i]['Date']\n",
    "                if pd.notna(date_val):\n",
    "                    date_dt = pd.to_datetime(date_val, errors='coerce')\n",
    "                    row['Date comptable'] =  date_dt.strftime(\"%d/%m/%Y\")\n",
    "                else:\n",
    "                    row['Date comptable'] = ''\n",
    "                \n",
    "                row['Montant'] = weekly_CB.iloc[i]['Montant total']\n",
    "                row['Description écriture'] = f\"{int(weekly_CB.iloc[i]['Numero Commande'])} CSPF  CB 28300120\"\n",
    "                row['Débit/Crédit'] = sens\n",
    "                row['Code Société'] = 1386\n",
    "                row['Centre de profit'] =  '33A840' \n",
    "                \n",
    "                # Compte comptable dépend du sens\n",
    "                if sens == 'D':\n",
    "                    row['Compte comptable'] = compte_debit\n",
    "\n",
    "                    if pd.notna(date_val):\n",
    "                        date_dt = pd.to_datetime(date_val, errors='coerce')\n",
    "                        row['Clé de lettrage'] = date_dt.strftime(\"%Y-%m-%d\")\n",
    "                        row['Affectation'] = date_dt.strftime(\"%Y-%m-%d\")\n",
    "                    else:\n",
    "                        row['Clé de lettrage'] = ''\n",
    "                        row['Affectation'] = ''\n",
    "\n",
    "\n",
    "                else:\n",
    "                    row['Compte comptable'] = compte_credit\n",
    "                    row['Clé de lettrage'] = 'SCC'\n",
    "                    row['Affectation'] = 'SCC'\n",
    "\n",
    "                maquette_data.append(row)\n",
    "\n",
    "    # Remplissage des OD VIOLETTE\n",
    "        credit_compteC=  2109000000\n",
    "        maquette_data1 = []\n",
    "\n",
    "        for sens in ['D', 'C']:\n",
    "            for i in range(len(weekly_hyper)):\n",
    "                row = {col: '' for col in colonnes}\n",
    "                date_val = weekly_hyper.iloc[i]['Date']\n",
    "                if pd.notna(date_val):\n",
    "                    date_dt = pd.to_datetime(date_val, errors='coerce')\n",
    "                    row['Date comptable'] = date_dt.strftime(\"%d/%m/%Y\")\n",
    "                else:\n",
    "                    row['Date comptable'] = ''\n",
    "                \n",
    "                row['Montant'] = weekly_hyper.iloc[i]['Montant total']\n",
    "                row['Description écriture'] = f\"{int(weekly_hyper.iloc[i]['Numero Commande'])} CSPF  CB CDE\"\n",
    "                row['Débit/Crédit'] = sens\n",
    "                row['Code Société'] = 1386\n",
    "                \n",
    "                if sens == 'C':\n",
    "                    row['Compte comptable'] = credit_compteC\n",
    "                    row['Clé de lettrage'] = 'SCC'\n",
    "                    row['Affectation'] = 'SCC'\n",
    "                    row['Centre de profit'] =  '33A840' \n",
    "                else:\n",
    "                    centre_cout = weekly_hyper.iloc[i]['Centre de coût-D']\n",
    "                    row['Centre de coût'] =  centre_cout \n",
    "                    compte_debit_val = weekly_hyper.iloc[i]['Compte comptable-D SAP']\n",
    "                    row['Compte comptable'] = compte_debit_val\n",
    "\n",
    "                #Code TVA selon le compte\n",
    "                    if compte_debit_val == 6150300000:\n",
    "                        row['Code TVA'] = 'A0'\n",
    "                    elif compte_debit_val == 6141000000:\n",
    "                        row['Code TVA'] = 'V0'\n",
    "                    else:\n",
    "                        row['Code TVA'] = ''\n",
    "            \n",
    "                maquette_data1.append(row)\n",
    "\n",
    "    # Remplissage des IRI \n",
    "        colonnes3 = [\n",
    "        \"Code évenement\",\"Date comptable (jj/mm/aaaa)\",\"Code de distribution\",\"Business Unit\", \"Code client\",\"Numéro de pièce\",\"Type de pièce\",\"Code comptable\",\"Opérating unit\",\"Code département\",\"Montant TTC\",\n",
    "        \"Date d'échéance(jj/mm/aaaa)\",\"Type de transaction\", \"Montant TVA\",\"Taux de TVA\",\"Commentaire\",\"Clé de lettrage\", \"Mode de paiement\", \"Adresse Mail pour envoie de la facture\"\n",
    "        ]\n",
    "\n",
    "        maquette_data2 = []\n",
    "\n",
    "        for i in range(len(weekly_NOIR)):\n",
    "            row = {col: '' for col in colonnes3}\n",
    "            date_val = weekly_NOIR.iloc[i]['Date']\n",
    "\n",
    "            if pd.notna(date_val):\n",
    "                date_dt = pd.to_datetime(date_val, errors='coerce')\n",
    "                row['Date comptable (jj/mm/aaaa)'] = date_dt.strftime(\"%d/%m/%Y\")\n",
    "            else:\n",
    "                row['Date comptable (jj/mm/aaaa)'] = ''\n",
    "\n",
    "\n",
    "            row['Code évenement'] = 'IRI'\n",
    "            row['Code de distribution'] = 'DC-RCVBL'\n",
    "            row['Business Unit'] = 'FR246'\n",
    "            row[\"Code client\"] = weekly_NOIR.iloc[i][\"Code client\"]\n",
    "            row[\"Numéro de pièce\"] = weekly_NOIR.iloc[i][\"Numero Commande\"]\n",
    "            row['Type de pièce'] = 'FACTU'\n",
    "            row['Code comptable'] = '21SCC' \n",
    "            row['Opérating unit'] = 'FRA903' \n",
    "            row['Code département'] = '03900' \n",
    "            row[\"Montant TTC\"] = weekly_NOIR.iloc[i][\"Montant total\"]\n",
    "            row['Type de transaction'] = 'SVSE'\n",
    "            row['Montant TVA'] = 0\n",
    "            row['Taux de TVA'] = 'ZER'\n",
    "            row[\"Commentaire\"] = weekly_NOIR.iloc[i][\"Numero Commande\"]\n",
    "            row['Clé de lettrage'] = 'SCC'   \n",
    "            \n",
    "            # Ajout d'1 mois à la date pour Date d'échéance\n",
    "            if pd.notna(date_val):\n",
    "                date_dt = pd.to_datetime(date_val, errors='coerce')\n",
    "                if pd.notna(date_dt):\n",
    "                    echeance = date_dt + relativedelta(months=1)\n",
    "                    row[\"Date d'échéance(jj/mm/aaaa)\"] = echeance.strftime(\"%d/%m/%Y\")\n",
    "                else:\n",
    "                    row[\"Date d'échéance(jj/mm/aaaa)\"] = ''\n",
    "\n",
    "            maquette_data2.append(row)\n",
    "\n",
    "        # Créer un dossier  pour acceuillir tout les  fichier  (VERT , VIOLET , IRI)\n",
    "        folder_vert_metadata = {\n",
    "            \"name\": f\"fichiers_du_{sem}\",\n",
    "            \"mimeType\": \"application/vnd.google-apps.folder\",\n",
    "            \"parents\": [TARGET_DRIVE_FOLDER_ID]\n",
    "        }\n",
    "        folder = drive_service.files().create(\n",
    "            body=folder_vert_metadata,\n",
    "            fields=\"id\",\n",
    "            supportsAllDrives=True\n",
    "        ).execute()\n",
    "\n",
    "        # Générer fichier VERT (weekly_CB)\n",
    "        maquette_finale_verte = pd.DataFrame(maquette_data, columns=colonnes)\n",
    "        buffer_verte = BytesIO()\n",
    "        maquette_finale_verte.to_excel(buffer_verte, index=False)\n",
    "        buffer_verte.seek(0)\n",
    "\n",
    "        file_metadata_verte = {\n",
    "            \"name\": f\"OD LIBRES O2C-CB {sem}.xlsx\",\n",
    "            \"parents\": [folder['id']],\n",
    "            \"mimeType\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "        }\n",
    "        media_verte = MediaIoBaseUpload(buffer_verte, mimetype=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "        drive_service.files().create(\n",
    "            body=file_metadata_verte,\n",
    "            media_body=media_verte,\n",
    "            fields=\"id\",\n",
    "            supportsAllDrives=True\n",
    "        ).execute()\n",
    "\n",
    "        logging.info(f\"Fichier VERT généré : OD LIBRES O2C-VERTE {sem}.xlsx\")\n",
    "\n",
    "        # Générer fichier VIOLET (DailySCC_hyper) \n",
    "        maquette_finale_violet = pd.DataFrame(maquette_data1, columns=colonnes)\n",
    "        buffer_violet = BytesIO()\n",
    "        maquette_finale_violet.to_excel(buffer_violet, index=False)\n",
    "        buffer_violet.seek(0)\n",
    "\n",
    "        file_metadata_violet = {\n",
    "            \"name\": f\"OD LIBRES O2C-HYPER {sem}.xlsx\",\n",
    "            \"parents\": [folder['id']],\n",
    "            \"mimeType\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "        }\n",
    "        media_violet = MediaIoBaseUpload(buffer_violet, mimetype=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "        drive_service.files().create(\n",
    "            body=file_metadata_violet,\n",
    "            media_body=media_violet,\n",
    "            fields=\"id\",\n",
    "            supportsAllDrives=True\n",
    "        ).execute()\n",
    "\n",
    "        logging.info(f\"Fichier VIOLET généré : OD LIBRES O2C-VIOLET {sem}.xlsx\")\n",
    "\n",
    "        #  Générer fichier IRI \n",
    "        maquette_finale_IRI = pd.DataFrame(maquette_data2, columns=colonnes3)\n",
    "        buffer_IRI = BytesIO()\n",
    "        maquette_finale_IRI.to_excel(buffer_IRI, index=False)\n",
    "        buffer_IRI.seek(0)\n",
    "\n",
    "        file_metadata_IRI = {\n",
    "            \"name\": f\"IRI_CRP_{sem}.xlsx\",\n",
    "            \"parents\": [folder['id']],\n",
    "            \"mimeType\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "        }\n",
    "        media_IRI = MediaIoBaseUpload(buffer_IRI, mimetype=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "        drive_service.files().create(\n",
    "            body=file_metadata_IRI,\n",
    "            media_body=media_IRI,\n",
    "            fields=\"id\",\n",
    "            supportsAllDrives=True\n",
    "        ).execute()\n",
    "\n",
    "        logging.info(f\"Fichier IRI généré : IRI_CRP_{sem}.xlsx\")\n",
    "\n",
    "        # Enregistrement de DailySCC_rouge\n",
    "        buffer_rouge = BytesIO()\n",
    "        fichier_weekly_Rouge.to_excel(buffer_rouge, index=False)\n",
    "        buffer_rouge.seek(0)\n",
    "\n",
    "        file_metadata_rouge = {\n",
    "            \"name\": f\"DailySCC_IG_{sem}.xlsx\",\n",
    "            \"parents\": [folder['id']],  \n",
    "            \"mimeType\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "        }\n",
    "\n",
    "        media_rouge = MediaIoBaseUpload(buffer_rouge, mimetype=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "\n",
    "        drive_service.files().create(\n",
    "            body=file_metadata_rouge,\n",
    "            media_body=media_rouge,\n",
    "            fields=\"id\",\n",
    "            supportsAllDrives=True\n",
    "        ).execute()\n",
    "\n",
    "        logging.info(f\"Fichier DailySCC_rouge généré : DailySCC_IG{sem}.xlsx\")\n",
    "\n",
    "        #  Enregistrement de DailySCC_carma\n",
    "        buffer_orange = BytesIO()\n",
    "        fichier_weekly_Orange.to_excel(buffer_orange, index=False)\n",
    "        buffer_orange.seek(0)\n",
    "\n",
    "        file_metadata_orange = {\n",
    "            \"name\": f\"DailySCC_CARMA_{sem}.xlsx\",\n",
    "            \"parents\": [folder['id']], \n",
    "            \"mimeType\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "        }\n",
    "\n",
    "        media_orange = MediaIoBaseUpload(buffer_orange, mimetype=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "\n",
    "        drive_service.files().create(\n",
    "            body=file_metadata_orange,\n",
    "            media_body=media_orange,\n",
    "            fields=\"id\",\n",
    "            supportsAllDrives=True\n",
    "        ).execute()\n",
    "\n",
    "        \n",
    "        logging.info(f\"Fichier DailySCC_CARMA généré : DailySCC_carma{sem}.xlsx\")\n",
    "\n",
    "        #  Enregistrement de DailySCC_CB\n",
    "        buffer_CB = BytesIO()\n",
    "        weekly_CB.to_excel(buffer_CB, index=False)\n",
    "        buffer_CB.seek(0)\n",
    "\n",
    "        file_metadata_CB = {\n",
    "            \"name\": f\"DailySCC_CB_{sem}.xlsx\",\n",
    "            \"parents\": [folder['id']], \n",
    "            \"mimeType\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "        }\n",
    "\n",
    "        media_CB = MediaIoBaseUpload(buffer_CB, mimetype=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "\n",
    "        drive_service.files().create(\n",
    "            body=file_metadata_CB,\n",
    "            media_body=media_CB,\n",
    "            fields=\"id\",\n",
    "            supportsAllDrives=True\n",
    "        ).execute()\n",
    "\n",
    "        \n",
    "        logging.info(f\"Fichier DailySCC_CB généré : DailySCC_CB{sem}.xlsx\")\n",
    "\n",
    "\n",
    "        #  Enregistrement de DailySCC_HYPER\n",
    "        buffer_hyper = BytesIO()\n",
    "        weekly_hyper.to_excel(buffer_hyper, index=False)\n",
    "        buffer_hyper.seek(0)\n",
    "\n",
    "        file_metadata_hyper = {\n",
    "            \"name\": f\"DailySCC_hyper_{sem}.xlsx\",\n",
    "            \"parents\": [folder['id']], \n",
    "            \"mimeType\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "        }\n",
    "\n",
    "        media_hyper = MediaIoBaseUpload(buffer_hyper, mimetype=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "\n",
    "        drive_service.files().create(\n",
    "            body=file_metadata_hyper,\n",
    "            media_body=media_hyper,\n",
    "            fields=\"id\",\n",
    "            supportsAllDrives=True\n",
    "        ).execute()\n",
    "\n",
    "        \n",
    "        logging.info(f\"Fichier DailySCC_hyper généré : DailySCC_hyper{sem}.xlsx\")\n",
    "\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lors du traitement du fichier {stream.name} (message ID: {msg_id}) : {e}\")\n",
    "\n",
    "\n",
    "if new_ids:\n",
    "    today_str = datetime.now().strftime(\"%Y/%m/%d\")\n",
    "    try:\n",
    "        save_state(drive_service, file_id, ids, today_str)\n",
    "        logging.info(f\"État mis à jour avec {len(new_ids)} nouveaux IDs et nouvelle date : {today_str}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lors de la sauvegarde de l'état : {e}\")\n",
    "\n",
    "\n",
    "logging.info(\"Fin du traitement.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
